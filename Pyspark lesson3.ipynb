{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Handling Missing Values\n",
    "#### Dropping Columns \n",
    "#### Dropping Rows\n",
    "#### Various Parameter In Dropping functionalities\n",
    "#### Handling Missing values by Mean, median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark =spark.read.csv('Data2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Temi|  21|        10| 30000|\n",
      "|   Jeff|  45|         3| 25000|\n",
      "|  Steve|  23|         4| 20000|\n",
      "|Cameron|  32|        12| 20000|\n",
      "| Sunny |  39|         2|150000|\n",
      "|  Mary |  25|         6|180000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  21|        10| 30000|\n",
      "|  45|         3| 25000|\n",
      "|  23|         4| 20000|\n",
      "|  32|        12| 20000|\n",
      "|  39|         2|150000|\n",
      "|  25|         6|180000|\n",
      "|null|      null| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## How to drop Columns \n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Temi|  21|        10| 30000|\n",
      "|   Jeff|  45|         3| 25000|\n",
      "|  Steve|  23|         4| 20000|\n",
      "|Cameron|  32|        12| 20000|\n",
      "| Sunny |  39|         2|150000|\n",
      "|  Mary |  25|         6|180000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Temi| 21|        10| 30000|\n",
      "|   Jeff| 45|         3| 25000|\n",
      "|  Steve| 23|         4| 20000|\n",
      "|Cameron| 32|        12| 20000|\n",
      "| Sunny | 39|         2|150000|\n",
      "|  Mary | 25|         6|180000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Temi| 21|        10| 30000|\n",
      "|   Jeff| 45|         3| 25000|\n",
      "|  Steve| 23|         4| 20000|\n",
      "|Cameron| 32|        12| 20000|\n",
      "| Sunny | 39|         2|150000|\n",
      "|  Mary | 25|         6|180000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### any == how\n",
    "df_pyspark.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Temi| 21|        10| 30000|\n",
      "|   Jeff| 45|         3| 25000|\n",
      "|  Steve| 23|         4| 20000|\n",
      "|Cameron| 32|        12| 20000|\n",
      "| Sunny | 39|         2|150000|\n",
      "|  Mary | 25|         6|180000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### any == how\n",
    "df_pyspark.na.drop(how = 'all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Temi|  21|        10| 30000|\n",
      "|   Jeff|  45|         3| 25000|\n",
      "|  Steve|  23|         4| 20000|\n",
      "|Cameron|  32|        12| 20000|\n",
      "| Sunny |  39|         2|150000|\n",
      "|  Mary |  25|         6|180000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## treshold\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Temi| 21|        10| 30000|\n",
      "|   Jeff| 45|         3| 25000|\n",
      "|  Steve| 23|         4| 20000|\n",
      "|Cameron| 32|        12| 20000|\n",
      "| Sunny | 39|         2|150000|\n",
      "|  Mary | 25|         6|180000|\n",
      "|   null| 34|        10| 38000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Subset drops null value oonly in a certain column\n",
    "df_pyspark.na.drop(how=\"any\", subset=['Experience'] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Temi|  21|        10| 30000|\n",
      "|   Jeff|  45|         3| 25000|\n",
      "|  Steve|  23|         4| 20000|\n",
      "|Cameron|  32|        12| 20000|\n",
      "| Sunny |  39|         2|150000|\n",
      "|  Mary |  25|         6|180000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filling the missing Value\n",
    "df_pyspark.na.fill('Missing Values', ['Experience', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Temi|  21|        10| 30000|\n",
      "|   Jeff|  45|         3| 25000|\n",
      "|  Steve|  23|         4| 20000|\n",
      "|Cameron|  32|        12| 20000|\n",
      "| Sunny |  39|         2|150000|\n",
      "|  Mary |  25|         6|180000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer \n",
    "imputer = Imputer(inputCols=['Age', 'Experience', 'Salary'],\n",
    "outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Experience', 'Salary']]).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Temi|  21|        10| 30000|         21|                10|         30000|\n",
      "|   Jeff|  45|         3| 25000|         45|                 3|         25000|\n",
      "|  Steve|  23|         4| 20000|         23|                 4|         20000|\n",
      "|Cameron|  32|        12| 20000|         32|                12|         20000|\n",
      "| Sunny |  39|         2|150000|         39|                 2|        150000|\n",
      "|  Mary |  25|         6|180000|         25|                 6|        180000|\n",
      "| Mahesh|null|      null| 40000|         31|                 6|         40000|\n",
      "|   null|  34|        10| 38000|         34|                10|         38000|\n",
      "|   null|  36|      null|  null|         36|                 6|         62875|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e06544754e6f7af70501e48cb6debe872c81eec92b9c0f92fe411ee3e379b1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
